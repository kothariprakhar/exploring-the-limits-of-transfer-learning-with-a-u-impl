{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)\n", "\n", "This notebook implements the core concept of the T5 paper: converting a classification problem (SST-2 Sentiment Analysis) into a text-to-text generation problem. We will fine-tune a pre-trained T5-small model to output 'positive' or 'negative' strings based on input sentences."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install transformers datasets torch scikit-learn matplotlib seaborn accelerate -q"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Imports and Configuration\n", "Defining the hyperparameters and model settings. We use `t5-small` for efficiency."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\nfrom datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport time\n\nclass Config:\n    MODEL_NAME = \"t5-small\"\n    BATCH_SIZE = 16\n    EPOCHS = 2\n    MAX_LEN_INPUT = 128\n    MAX_LEN_TARGET = 5 # \"positive\" or \"negative\" are short\n    LEARNING_RATE = 2e-4\n    SEED = 42\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntorch.manual_seed(Config.SEED)\nprint(f\"Using device: {Config.DEVICE}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Data Processing: The Text-to-Text Paradigm\n", "Here we implement the key contribution of the paper. Instead of returning integer labels (0 or 1), we process the data to return target strings. We also add the task prefix `sst2 sentence:` to the inputs."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_and_process_data():\n    print(\"Loading SST-2 dataset...\")\n    dataset = load_dataset(\"glue\", \"sst2\")\n    \n    # Map integer labels to text strings\n    label_map = {0: \"negative\", 1: \"positive\"}\n    \n    tokenizer = T5Tokenizer.from_pretrained(Config.MODEL_NAME, legacy=False)\n\n    def preprocess_function(examples):\n        # T5 specific prefix for task specification\n        inputs = [f\"sst2 sentence: {sentence}\" for sentence in examples[\"sentence\"]]\n        targets = [label_map[label] for label in examples[\"label\"]]\n        \n        # Tokenize inputs\n        model_inputs = tokenizer(inputs, max_length=Config.MAX_LEN_INPUT, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        \n        # Tokenize targets\n        labels = tokenizer(targets, max_length=Config.MAX_LEN_TARGET, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        \n        # Replace padding token id's of the labels by -100 so it's ignored by the loss\n        labels = labels[\"input_ids\"]\n        labels[labels == tokenizer.pad_token_id] = -100\n        \n        model_inputs[\"labels\"] = labels\n        return model_inputs\n\n    # Apply processing\n    tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"sentence\", \"label\", \"idx\"])\n    tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n    \n    return tokenized_datasets, tokenizer\n\ntokenized_datasets, tokenizer = load_and_process_data()\n\n# Using a subset for demonstration speed in the Notebook context\ntrain_loader = DataLoader(tokenized_datasets[\"train\"].shuffle(seed=Config.SEED).select(range(2000)), batch_size=Config.BATCH_SIZE)\nval_loader = DataLoader(tokenized_datasets[\"validation\"], batch_size=Config.BATCH_SIZE)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Model Initialization & Training Loop\n", "We initialize the `T5ForConditionalGeneration` model. The training loop uses standard backpropagation. Note that T5 automatically handles label shifting (teacher forcing) internally."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = T5ForConditionalGeneration.from_pretrained(Config.MODEL_NAME)\nmodel.to(Config.DEVICE)\n\noptimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\ntotal_steps = len(train_loader) * Config.EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\nprint(\"Starting Training...\")\ntrain_losses = []\n\nfor epoch in range(Config.EPOCHS):\n    model.train()\n    total_loss = 0\n    start_time = time.time()\n    \n    for step, batch in enumerate(train_loader):\n        input_ids = batch[\"input_ids\"].to(Config.DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(Config.DEVICE)\n        labels = batch[\"labels\"].to(Config.DEVICE)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.item()\n        \n        if step % 50 == 0:\n            print(f\"Epoch {epoch+1} | Step {step} | Loss: {loss.item():.4f}\")\n            \n    avg_loss = total_loss / len(train_loader)\n    train_losses.append(avg_loss)\n    print(f\"Epoch {epoch+1} Complete. Avg Loss: {avg_loss:.4f}. Time: {time.time() - start_time:.2f}s\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Evaluation and Visualization\n", "We generate predictions using the model and compare strings. We visualize the training curve and the confusion matrix."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Starting Evaluation...\")\nmodel.eval()\npredictions = []\nactuals = []\n\nwith torch.no_grad():\n    for batch in val_loader:\n        input_ids = batch[\"input_ids\"].to(Config.DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(Config.DEVICE)\n        \n        generated_ids = model.generate(\n            input_ids=input_ids, \n            attention_mask=attention_mask, \n            max_length=Config.MAX_LEN_TARGET\n        )\n        \n        preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n        predictions.extend(preds)\n        \n        lbls = batch[\"labels\"].cpu().numpy()\n        lbls = np.where(lbls != -100, lbls, tokenizer.pad_token_id)\n        refs = tokenizer.batch_decode(lbls, skip_special_tokens=True)\n        actuals.extend(refs)\n\npredictions = [p.strip() for p in predictions]\nactuals = [a.strip() for a in actuals]\n\nacc = accuracy_score(actuals, predictions)\nprint(f\"Validation Accuracy: {acc:.4f}\")\n\n# --- PLOT 1: Training Loss ---\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, Config.EPOCHS + 1), train_losses, marker='o', label='Training Loss')\nplt.title(\"T5 Fine-Tuning Loss on SST-2\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# --- PLOT 2: Confusion Matrix ---\nunique_labels = sorted(list(set(actuals + predictions)))\ncm = confusion_matrix(actuals, predictions, labels=unique_labels)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\nplt.title(\"Confusion Matrix: Text-to-Text Classification\")\nplt.xlabel(\"Predicted Text\")\nplt.ylabel(\"Actual Text\")\nplt.show()\n\n# --- Display Samples ---\ndf_results = pd.DataFrame({\"Actual\": actuals, \"Predicted\": predictions})\nprint(\"Sample Predictions:\")\nprint(df_results.head(10))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}